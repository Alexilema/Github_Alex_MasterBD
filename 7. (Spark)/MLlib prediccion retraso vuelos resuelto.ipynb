{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# MLlib hands-on: RandomForest para predecir la severidad del retraso"}, {"cell_type": "markdown", "metadata": {}, "source": "## Descripci\u00f3n de las variables"}, {"cell_type": "markdown", "metadata": {}, "source": "El dataset est\u00e1 compuesto por las siguientes variables:\n\n1. **Year** 2008\n2. **Month** 1\n3. **DayofMonth** 1-31\n4. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n5. **DepTime** hora real de salida (local, hhmm)\n6. **CRSDepTime** hora prevista de salida (local, hhmm)\n7. **ArrTime** hora real de llegada (local, hhmm)\n8. **CRSArrTime** hora prevista de llegada (local, hhmm)\n9. **UniqueCarrier** c\u00f3digo del aparato\n10. **FlightNum** n\u00famero de vuelo\n11. **TailNum** identificador de cola: aircraft registration, unique aircraft identifier\n12. **ActualElapsedTime** tiempo real invertido en el vuelo\n13. **CRSElapsedTime** en minutos\n14. **AirTime** en minutos\n15. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n16. **DepDelay** retraso a la salida, en minutos\n17. **Origin** c\u00f3digo IATA del aeropuerto de origen\n18. **Dest** c\u00f3digo IATA del aeropuerto de destino\n19. **Distance** en millas\n20. **TaxiIn** taxi in time, in minutes\n21. **TaxiOut** taxi out time in minutes\n22. **Cancelled** *si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n23. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n24. **Diverted** *si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n25. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n26. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n27. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n28. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n29. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista."}, {"cell_type": "markdown", "metadata": {}, "source": "**IMPORTANTE:** Vamos a emplear una versi\u00f3n reducida del dataset de vuelos para entender la forma de trabajar con MLlib, y en particular, los mecanimos de exploraci\u00f3n de combinaciones de h\u00edper-par\u00e1metros para encontrar el mejor modelo, los cuales requieren ajustar varios modelos distintos varias veces. Este proceso ser\u00eda demasiado largo si utiliz\u00e1semos el dataset original de 300 MB. \n\n<p>En la siguiente celda, descargaremos este dataset reducido de Internet mediante la ejecuci\u00f3n del comandos de Linux `wget` y lo subiremos a HDFS  con el comando de HDFS `copyFromLocal`. Los datos guardados en HDFS son vol\u00e1tiles y desaparecer\u00e1n cuando el cluster sea desmantelado.</p>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Vamos a utilizar una versi\u00f3n reducida del dataset original para poder ver en funcionamiento el ajuste de hiper-par\u00e1metros, que requiere ajustar varios modelos"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Descargamos el CSV reducido y lo subimos a HDFS (esta vez no es Google Cloud Storage)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--2022-07-01 16:59:40--  https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flights_jan08.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9719583 (9.3M) [text/plain]\nSaving to: \u2018flights_jan08.csv\u2019\n\nflights_jan08.csv   100%[===================>]   9.27M  --.-KB/s    in 0.07s   \n\n2022-07-01 16:59:40 (142 MB/s) - \u2018flights_jan08.csv\u2019 saved [9719583/9719583]\n\n"}], "source": "!wget https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flights_jan08.csv\n!hdfs dfs -copyFromLocal flights_jan08.csv /tmp"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Ahora lo leemos desde la ruta /tmp/flights_jan08.csv de HDFS"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "# Leemos el dataset de HDFS. Esta operaci\u00f3n todav\u00eda no hace la lectura hace una pasada \n# sobre los datos para inferir el esquema\nflightsDF = spark.read.option(\"header\", \"true\")\\\n                      .option(\"inferSchema\", \"true\")\\\n                      .csv(\"/tmp/flights_jan08.csv\")"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": "100000"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.count()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Year: integer (nullable = true)\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- DepTime: string (nullable = true)\n |-- CRSDepTime: integer (nullable = true)\n |-- ArrTime: string (nullable = true)\n |-- CRSArrTime: integer (nullable = true)\n |-- UniqueCarrier: string (nullable = true)\n |-- FlightNum: integer (nullable = true)\n |-- TailNum: string (nullable = true)\n |-- ActualElapsedTime: string (nullable = true)\n |-- CRSElapsedTime: integer (nullable = true)\n |-- AirTime: string (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- DepDelay: string (nullable = true)\n |-- Origin: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- Distance: integer (nullable = true)\n |-- TaxiIn: string (nullable = true)\n |-- TaxiOut: string (nullable = true)\n |-- Cancelled: integer (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: integer (nullable = true)\n |-- CarrierDelay: string (nullable = true)\n |-- WeatherDelay: string (nullable = true)\n |-- NASDelay: string (nullable = true)\n |-- SecurityDelay: string (nullable = true)\n |-- LateAircraftDelay: string (nullable = true)\n\n"}], "source": "flightsDF.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a utilizar algunos transformadores habituales, y un algoritmo RandomForest que es un estimador. Utilizaremos:\n\n* StringIndexer para convertir variables tipo String en variables categ\u00f3ricas pero cuyos valores son n\u00fameros reales con la parte decimal a 0, tal como necesitan los algoritmos de Spark.\n* Bucketizer para discretizar la columna de ArrDelay sin dar nombre a las categor\u00edas, solo numeros. Ser\u00e1 nuestra variable target.\n* VectorAssembler para unir las columnas de las features en una sola de tipo vector\n* RandomForest que es un estimador, como algoritmo de predicci\u00f3n de la severidad\n* Pipeline, que es un estimador y que incluir\u00e1 todos los elementos anteriores."}, {"cell_type": "markdown", "metadata": {}, "source": "### \u00bfC\u00f3mo har\u00edamos para construir tantos StringIndexer como columnas de string tengamos?"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"data": {"text/plain": "[('Year', 'int'),\n ('Month', 'int'),\n ('DayofMonth', 'int'),\n ('DayOfWeek', 'int'),\n ('DepTime', 'string'),\n ('CRSDepTime', 'int'),\n ('ArrTime', 'string'),\n ('CRSArrTime', 'int'),\n ('UniqueCarrier', 'string'),\n ('FlightNum', 'int'),\n ('TailNum', 'string'),\n ('ActualElapsedTime', 'string'),\n ('CRSElapsedTime', 'int'),\n ('AirTime', 'string'),\n ('ArrDelay', 'string'),\n ('DepDelay', 'string'),\n ('Origin', 'string'),\n ('Dest', 'string'),\n ('Distance', 'int'),\n ('TaxiIn', 'string'),\n ('TaxiOut', 'string'),\n ('Cancelled', 'int'),\n ('CancellationCode', 'string'),\n ('Diverted', 'int'),\n ('CarrierDelay', 'string'),\n ('WeatherDelay', 'string'),\n ('NASDelay', 'string'),\n ('SecurityDelay', 'string'),\n ('LateAircraftDelay', 'string')]"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.dtypes"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "from pyspark.ml.feature import StringIndexer\n\ncategoricas = [nombre for (nombre, tipo) in flightsDF.dtypes if tipo == \"string\"]\nlista_string_indexers = [StringIndexer(inputCol=c, outputCol=c+\"Indexed\", handleInvalid=\"keep\") for c in categoricas]"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "from pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.ml.feature import Bucketizer, StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\n\ncleanFlightsDF = flightsDF.where(\"ArrDelay != 'NA' and DepDelay != 'NA' and DepTime != 'NA' and ArrTime != 'NA'\")\\\n                          .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(IntegerType()))\\\n                          .withColumn(\"DepDelay\", F.col(\"DepDelay\").cast(IntegerType()))\\\n                          .withColumn(\"ArrTime\", F.col(\"ArrTime\").cast(IntegerType()))\\\n                          .withColumn(\"DepTime\", F.col(\"DepTime\").cast(IntegerType())).cache()\n\n# Definimos tres categor\u00edas: <15, entre 15 y 60, >60. Cada una se codifica con un n\u00famero real: 0.0, 1.0, 2.0 \nsplitsDelays = [-float(\"inf\"), 15, 60, float(\"inf\")]\narrDelayBucketizer = Bucketizer(splits=splitsDelays, inputCol=\"ArrDelay\", outputCol=\"ArrDelayBucketed\")\n\n# Definimos varias franjas: 00:00 - 06:00, 06:00 - 12:00, 12:00 - 18:00, 18:00 - 22:00, 22:00 - 00:00\nsplitsDepTime = [-1, 600, 1200, 1800, 2200, 2500]\ndepTimeBucketizer = Bucketizer(splits=splitsDepTime, inputCol=\"DepTime\", outputCol=\"DepTimeBucketed\")\n\n# Dividimos en train y test de manera aleatoria usando la semilla 123 para los n\u00fameros aleatorios, para que sea reproducible. \n# Esto devuelve una lista de dos DataFrames. La divisi\u00f3n har\u00e1 que el primer elemento de la lista sea un DF \n# con aprox. el 70 % de las filas. Lo usaremos para entrenar. El otro DF tendr\u00e1 aprox el 30 % restante de las filas. \n\nsplits = cleanFlightsDF.randomSplit([0.7, 0.3], seed = 123)\ntrainDF = splits[0].cache() # El primer DF tiene el 70 % de los datos\ntestDF = splits[1].cache()\n\n# Si quisiera generar la lista con todos los StringIndexer para TODAS las columnas:\ncategoricas = [\"asd\u00f1fkj\", \"a\u00f1dslkjadf\"]\n#indexerList = [StringIndexer(inputCol=c, outputCol=c + \"indexed\", handleInvalid=\"skip\") for c in categoricas]\n\n# Indexamos las columnas categ\u00f3ricas Origin, Dest y DayOfWeek, para traducirlas a reales con la parte decimal a 0.\n# Recordemos que esto tambi\u00e9n introduce metadatos adicionales en la columna resultante para indicar que, aunque sea\n# una columna de n\u00fameros reales, en realidad est\u00e1n representando categor\u00edas y debe ser tratada como tal por el algoritmo\noriginIndexer = StringIndexer(inputCol = \"Origin\", outputCol=\"OriginIndexed\", handleInvalid=\"skip\")\ndestIndexer = StringIndexer(inputCol = \"Dest\", outputCol=\"DestIndexed\", handleInvalid=\"skip\")\ndowIndexer = StringIndexer(inputCol = \"DayOfWeek\", outputCol=\"DayOfWeekIndexed\", handleInvalid=\"skip\")\n\n# Ahora unimos todas las columnas que se usar\u00e1n como variables en una sola columna de tipo vector llamada featuresVector\nvectorAssembler = VectorAssembler(inputCols = [\"DepDelay\", \"DepTimeBucketed\", \"OriginIndexed\", \"DestIndexed\", \"DayOfWeekIndexed\"], \n                                  outputCol = \"featuresVector\")\n\nrandomForest = RandomForestClassifier(featuresCol = \"featuresVector\",\n                                      labelCol = \"ArrDelayBucketed\",\n                                      numTrees = 50, maxBins=100)\n\npipeline = Pipeline(stages=[depTimeBucketizer, arrDelayBucketizer, dowIndexer, originIndexer, destIndexer, \n                            vectorAssembler, randomForest])"}, {"cell_type": "markdown", "metadata": {}, "source": "Aplicamos fit para ajustar el pipeline completo. Lo que esto hace es, para cada etapa:\n- Si la etapa es un Transformer, invoca al m\u00e9todo transform() pas\u00e1ndole el DF que recibe de la etapa anterior,\n  y env\u00eda el resultado (DF transformado) a la etapa siguiente del pipeline.\n- Si la etapa es un Estimator, invoca al m\u00e9todo fit() pas\u00e1ndole como argumento el DF recibido de la etapa anterior,\n  y despu\u00e9s invoca transform() sobre el objeto devuelto por fit, pasando de nuevo dicho DF como argumento. \n  Finalmente el DF devuelvo por transform es enviado a la etapa siguiente del pipeline."}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "pipelineModel = pipeline.fit(trainDF)"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a hacer predicciones sobre los datos de test. Esto devolver\u00e1 un nuevo DF al que se le han a\u00f1adido varias columnas al final:\n1. **`rawPrediction`**: magnitud que tiene una interpretaci\u00f3n diferente seg\u00fan el algoritmo pero que intuitivamente nos da una\n  medida de la confianza en cada posible clase (cuanto mayor, m\u00e1s confianza se tiene en que esa es la clase del ejemplo). En\n  nuestro caso ser\u00e1 un vector de 3 n\u00fameros reales puesto que nuestro problema tiene 3 clases\n2. **`probability`**: vector de probabilidades de que el ejemplo pertenezca a cada una de las 3 clases de nuestro problema\n3. **`prediction`**: clase para la cual la rawProbability es mayor."}, {"cell_type": "markdown", "metadata": {}, "source": "## Explicar qu\u00e9 ocurre cuando no viene la columna ArrDelay en el DataFrame de datos para predicci\u00f3n"}, {"cell_type": "markdown", "metadata": {}, "source": "En un conjunto de datos reales a los cuales tengamos que hacerles predicciones, lo normal es que no tengamos ninguna columna target (ArrDelay) porque desconocemos ese dato y precisamente por eso querr\u00edamos predecirlo. Sin embargo, en nuestro pipeline hemos incluido la etapa `arrDelayBucketizer` que discretiza la variable ArrDelay. Esto significa que cualquier DataFrame que queramos predecir (`transform(...)`) con el pipeline entrenado tiene que tener una columna llamada ArrDelay para que la pieza `arrDelayBucketizer` no reviente y pueda funcionar con normalidad, **a pesar de que dicha columna ArrDelay no va a ser utilizada por nadie m\u00e1s en el pipeline a la hora de predecir**, ya que naturalmente, el RandomForestClassificationModel (modelo ya entrenado) no necesita ninguna variable target para calcular predicciones. Por tanto tenemos dos opciones:\n* Incluir una variable ArrDelay ficticia, inventada, en el DataFrame de datos para predecir, con cualquier valor. La finalidad es simplemente **evitar** que la pieza `arrDelayBucketizer` (y por tanto, el pipeline completo) reviente al hacer predicciones.\n* Quitar del pipeline la pieza `arrDelayBucketizer` y ajustarla **por fuera del pipeline**, antes incluso de hacer la divisi\u00f3n en train y test. De esta manera, como el pipeline no contendr\u00e1 ninguna pieza que vaya a requerir una columna ArrDelay, no ser\u00e1 problema que el DataFrame con el que predecimos no incluya esa columna."}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "predictionsDF = pipelineModel.transform(testDF)\n\n#pipelineModel.transform(realDataDF.withColumn(\"ArrDelay\", F.lit(999)))"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Hay 69267 ejemplos de entrenamiento\n"}], "source": "print(\"Hay {0} ejemplos de entrenamiento\".format(trainDF.count()))"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Year: integer (nullable = true)\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- CRSDepTime: integer (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- CRSArrTime: integer (nullable = true)\n |-- UniqueCarrier: string (nullable = true)\n |-- FlightNum: integer (nullable = true)\n |-- TailNum: string (nullable = true)\n |-- ActualElapsedTime: string (nullable = true)\n |-- CRSElapsedTime: integer (nullable = true)\n |-- AirTime: string (nullable = true)\n |-- ArrDelay: integer (nullable = true)\n |-- DepDelay: integer (nullable = true)\n |-- Origin: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- Distance: integer (nullable = true)\n |-- TaxiIn: string (nullable = true)\n |-- TaxiOut: string (nullable = true)\n |-- Cancelled: integer (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: integer (nullable = true)\n |-- CarrierDelay: string (nullable = true)\n |-- WeatherDelay: string (nullable = true)\n |-- NASDelay: string (nullable = true)\n |-- SecurityDelay: string (nullable = true)\n |-- LateAircraftDelay: string (nullable = true)\n |-- DepTimeBucketed: double (nullable = true)\n |-- ArrDelayBucketed: double (nullable = true)\n |-- DayOfWeekIndexed: double (nullable = false)\n |-- OriginIndexed: double (nullable = false)\n |-- DestIndexed: double (nullable = false)\n |-- featuresVector: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n"}], "source": "predictionsDF.printSchema()"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------------------------------------+------------------------------------------------------------+----------+\n|rawPrediction                                             |probability                                                 |prediction|\n+----------------------------------------------------------+------------------------------------------------------------+----------+\n|[14.503102021930536,33.57778245644096,1.9191155216285107] |[0.29006204043861067,0.671555649128819,0.03838231043257021] |1.0       |\n|[9.409616208265614,37.46101389035216,3.1293699013822276]  |[0.18819232416531226,0.7492202778070431,0.06258739802764454]|1.0       |\n|[8.814622695575846,37.811535493570105,3.3738418108540524] |[0.17629245391151688,0.756230709871402,0.06747683621708105] |1.0       |\n|[9.070472048617143,37.68035029330417,3.2491776580786955]  |[0.18140944097234282,0.7536070058660833,0.0649835531615739] |1.0       |\n|[8.590656287665485,38.299744706884994,3.1095990054495357] |[0.17181312575330965,0.7659948941376996,0.0621919801089907] |1.0       |\n|[18.486981392448904,29.89382286691687,1.6191957406342252] |[0.36973962784897807,0.5978764573383374,0.0323839148126845] |1.0       |\n|[8.315056090326985,38.662584531061555,3.0223593786114686] |[0.1663011218065397,0.773251690621231,0.06044718757222937]  |1.0       |\n|[8.18504629185278,38.360010717699524,3.454942990447704]   |[0.1637009258370556,0.7672002143539903,0.06909885980895407] |1.0       |\n|[15.136160071035581,32.71600460170256,2.147835327261863]  |[0.3027232014207116,0.6543200920340511,0.042956706545237255]|1.0       |\n|[18.88856632527053,29.41871354363068,1.692720131098789]   |[0.3777713265054106,0.5883742708726136,0.03385440262197578] |1.0       |\n|[8.511571863232254,36.044901772569915,5.443526364197844]  |[0.170231437264645,0.7208980354513981,0.10887052728395684]  |1.0       |\n|[19.57243301927689,28.6537012348067,1.7738657459164069]   |[0.39144866038553777,0.573074024696134,0.03547731491832814] |1.0       |\n|[15.511356120715279,32.76818643097706,1.720457448307665]  |[0.31022712241430556,0.6553637286195412,0.0344091489661533] |1.0       |\n|[8.773306653899867,37.831963757954,3.394729588146142]     |[0.17546613307799733,0.7566392751590799,0.06789459176292283]|1.0       |\n|[8.818892199125157,37.83958492616438,3.341522874710465]   |[0.17637784398250314,0.7567916985232875,0.0668304574942093] |1.0       |\n|[8.746782398586848,38.22513700585577,3.0280805955573826]  |[0.17493564797173697,0.7645027401171154,0.06056161191114765]|1.0       |\n|[15.475823992438809,32.781078633192884,1.7430973743682987]|[0.3095164798487762,0.6556215726638578,0.03486194748736598] |1.0       |\n|[11.566496897208566,35.966994774692395,2.466508328099056] |[0.23132993794417125,0.7193398954938477,0.04933016656198111]|1.0       |\n|[8.958963580898983,37.82076290937164,3.220273509729385]   |[0.17917927161797964,0.7564152581874326,0.0644054701945877] |1.0       |\n|[8.90573551126914,37.790850196797706,3.3034142919331635]  |[0.17811471022538278,0.7558170039359539,0.06606828583866325]|1.0       |\n+----------------------------------------------------------+------------------------------------------------------------+----------+\nonly showing top 20 rows\n\n"}], "source": "predictionsDF.select(\"rawPrediction\", \"probability\", \"prediction\").where(\"prediction = 1.0\").show(truncate=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Evaluaci\u00f3n del modelo (evaluamos las predicciones que hemos hecho sobre el DF de test)"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test Error = 0.0769649 \n"}], "source": "# Select (prediction, true label) and compute test error\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Objeto evaluador, para evaluar las predicciones. Compara una columna de predicciones con la columna target del verdadero valor\n# Hay varias m\u00e9tricas posibles, pero nosotros hemos elegido la m\u00e1s simple: porcentaje de acierto en clasificaci\u00f3n.\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"ArrDelayBucketed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n\naccuracy = evaluator.evaluate(predictionsDF)\n\nprint(\"Test Error = %g \" % (1.0 - accuracy))"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"data": {"text/plain": "RandomForestClassificationModel (uid=RandomForestClassifier_2663fa75fe2c) with 50 trees"}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "pipelineModel.stages[-1]"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/plain": "SparseVector(5, {0: 0.9644, 1: 0.0228, 2: 0.0029, 3: 0.0035, 4: 0.0065})"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "pipelineModel.stages[-1].featureImportances"}, {"cell_type": "markdown", "metadata": {}, "source": "### Ajuste de h\u00edper-par\u00e1metros utilizando Cross Validation sobre el subconjunto de train"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"data": {"text/plain": "50"}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": "randomForest.getOrDefault(\"numTrees\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# Objeto ParamGrid al que le tenemos que indicar cada uno de los par\u00e1metros sobre los que queremos buscar,\n# y la lista de valores posibles que queremos probar con cada uno. Deben ser par\u00e1metros tomados directamente\n# del objeto estimador que se a\u00f1adi\u00f3 al pipeline (no pueden ser de ning\u00fan otro modelo). En nuestro caso este objeto\n# est\u00e1 almacenado en la variable randomForest que hab\u00edamos creado en celdas anteriores.\nparamGrid = ParamGridBuilder()\\\n    .addGrid(randomForest.numTrees, [50, 100, 150])\\\n    .addGrid(randomForest.maxDepth, [3, 4, 5])\\\n    .build() # como tenemos 2 hper-par\u00e1metros con 3 valores posibles cada uno, hay 9 combinaciones posibles\n\n# CrossValidator es un estimador. Cuando invocamos a fit(), probar\u00e1 todas las combinaciones de valores de los \n# par\u00e1metros, invocando con cada combinaci\u00f3n al m\u00e9todo fit() del objeto pipeline que le hemos pasado como argumento\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)\n\n# Como hemos hecho 3 folds, habr\u00e1 que entrenar 3 veces cada modelo candidato (cada combinaci\u00f3n de par\u00e1metros)\n# y obtener su media de accuracy. En total vamos a entrenar 27 modelos\n\ncvModel = crossval.fit(trainDF)\ncvModel.bestModel"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "prediccionesMejorDF = cvModel.bestModel.transform(testDF)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### El objeto RandomForestModel (modelo ajustado presente en la \u00faltima etapa del pipeline ya ajustado) dispone de ciertos atajos para recuperar valores de algunos par\u00e1metros (por ejemplo getNumTrees) pero no de otros (por ejemplo, no existe getMaxDepth)"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "N\u00famero \u00f3ptimo de \u00e1rboles: 150\nMax depth \u00f3ptimo: 5\n"}], "source": "rf = cvModel.bestModel.stages[6]\nprint(\"N\u00famero \u00f3ptimo de \u00e1rboles: {0}\".format(rf.getNumTrees))\nprint(\"Max depth \u00f3ptimo: {0}\".format(rf.getOrDefault('maxDepth')))"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}